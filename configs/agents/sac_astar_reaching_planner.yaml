algo_name: 'sac_astar'
seed: 10
env_name: 'reaching-v0' # remember to switch script
sac_steps_per_epoch: 50000
sac_epochs: 100
replay_size: 1000000 # for cluttered, set to 500000
gamma: 0.95 # need to adjust to 0.99 when using cluttered env
polyak: 0.95
lr: 0.001
alpha: 0.2
batch_size: 1024
start_steps: 1000
update_after: 1000
update_every: 2000
save_freq: 10
num_test_episodes: 100 # for cluttered env set to 10
log_dir: 'runs_rl/'
logging_dir: 'runs_rl/'
output_fname: 'experiment_document.txt'
activation: 'ReLU'
hidden_sizes: [512, 512, 512]
whether_her: false
use_auto: true
whether_astar: false
astar_ablation: false # true stands for ablation study
astar_mp_steps: 10
astar_N_steps: 10
astar_max_iter: 50
astar_heuristic_type: 'traditional'
use_logger: false


pretrained: False
pretrained_itr: 2499999
pretrained_dir: 'runs_rl/reaching-v0_sac_astar_three_trailer_60_20240122_223222/'

env_config:
  allow_backward: true
  distance_matrix: [1.00, 1.00, 1.00, 1.00, 1.00, 1.00]
  reward_weights: [1, 0.3, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02]
  max_episode_steps: 40
  N_steps: 10
  controlled_vehicle_config:
    max_steer: 0.6
    rb: 1.0
    rf: 4.5
    rtb: 3.0
    rtb2: 3.0
    rtb3: 3.0
    rtf: 1.0
    rtf2: 1.0
    rtf3: 1.0
    rtr: 2.0
    rtr2: 2.0
    rtr3: 2.0
    safe_d: 0.0
    tr: 0.5
    tw: 1.0
    v_max: 2.0
    w: 2.0
    wb: 3.5
    wd: 1.4
    xi_max: 0.7853981633974483
    safe_d: 0.0
    safe_metric: 3.0
  evaluate_mode: true
  goal_region_bound:
    x_min: -30
    x_max: 30
    y_min: -30
    y_max: 30
  outer_wall_bound:
    x_min: -50
    x_max: 50
    y_min: -50
    y_max: 50 
  reward_type: sparse_reward_mod
  start_region_bound:
    x_min: 0
    x_max: 0
    y_min: 0
    y_max: 0
  sucess_goal_reward_sparse: 15
  sparse_reward_threshold: 0.5
  vehicle_type: three_trailer
  verbose: false
  jack_knife_penalty: -20
  collision_penalty: -20
  use_rgb: false
  number_obstacles: 0
  max_length: 20
  min_length: 1
  observation: original
